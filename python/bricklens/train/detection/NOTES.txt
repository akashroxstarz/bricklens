Current status:

- Model loading and instantiation should work
- Got training code up and running.
- Generated dataset in the simple format.

To do:

- Map classes to integer values.
- Make sure number of output classes matches number of classes in dataset.
- Look at assumption of 416 x 416 images.
- Consider starting from existing Yolo3 pretrained weights rather than training from scratch?
- Look at anchor boxes - they are almost certainly not right.


Notes on YOLO3 model code:

The config file "yolov3.cfg" is from PJReddie's code here:

https://github.com/pjreddie/darknet/blob/master/cfg/yolov3-voc.cfg

The "VOC" variant takes 416x416 input images. It differs in terms of
the image size used, number of classes (80 vs 20), and the "ignore_thresh"
parameter (0.7 vs 0.5). So I think this is not that meaningful.

Looks like the code I'm using (from pytorch_custom_yolo_training repo)
has modified it to only support 3 models.

The version I'm using has these changes:
  - Batch size of 16 (vs 64 in the original yolov3-voc.cfg).
  - LR of 0.002 (vs 0.001).
  - Output classes = 3 (vs 20).

Network looks like this:

  conv2d 3x3, 32 channels, stride 1 + bn + leaky

  conv2d 3x3, 64 channels, stride 2 + bn + leaky
  conv2d 1x1, 32 channels, stride 1 + bn + leaky
  conv2d 3x3, 64 channels, stride 1 + bn + leaky
  shortcut from -3

  conv2d 3x3, 128 channels, stride 2 + bn + leaky
  conv2d 1x1, 64 channels, stride 1 + bn + leaky
  conv2d 3x3, 128 channels, stride 1 + bn + leaky
  shortcut from -3

  conv2d 1x1, 64 channels, stride 1 + bn + leaky
  conv2d 3x3, 128 channels, stride 1 + bn + leaky
  shortcut from -3

  conv2d 3x3, 256 channels, stride 2 + bn + leaky
  conv2d 1x1, 128 channels, stride 1 + bn + leaky
  conv2d 3x3, 256 channels, stride 1 + bn + leaky
  shortcut from -3

  conv2d 1x1, 128 channels, stride 1 + bn + leaky |
  conv2d 3x3, 256 channels, stride 1 + bn + leaky |  x 7
  shortcut from -3                                |

  conv2d 3x3, 512 channels, stride 2 + bn + leaky
  conv2d 1x1, 256 channels, stride 1 + bn + leaky
  conv2d 3x3, 512 channels, stride 1 + bn + leaky
  shortcut from -3
  
  conv2d 1x1, 256 channels, stride 1 + bn + leaky |
  conv2d 3x3, 512 channels, stride 1 + bn + leaky |  x 7
  shortcut from -3                                |

  conv2d 3x3, 1024 channels, stride 2 + bn + leaky
  conv2d 1x1, 512 channels, stride 1 + bn + leaky
  conv2d 3x3, 1024 channels, stride 1 + bn + leaky
  shortcut from -3
  
  conv2d 1x1, 512 channels, stride 1 + bn + leaky  |
  conv2d 3x3, 1024 channels, stride 1 + bn + leaky |  x 3
  shortcut from -3                                 |

  #### YOLO output #1:

  conv2d 1x1, 512 channels, stride 1 + bn + leaky  |
  conv2d 3x3, 1024 channels, stride 1 + bn + leaky |  x 3

  conv2d 1x1, 24 channels, no BN, linear

  yolo (mask = 6, 7, 8) classes=3 num=9  - The "mask" seems to have to do with which anchors are used.
    There are 9 anchor boxes in total, this is selecting the last 3 of them.

  route from -4

  conv2d 1x1, 256 channels, stride 1 + bn + leaky

  upsample stride=2

  route from layers -1 and 61 (???)

  #### YOLO output #2:

  conv2d 1x1, 256 channels, stride 1 + bn + leaky |
  conv2d 3x3, 512 channels, stride 1 + bn + leaky |  x 3

  conv2d 1x1, 24 channels, no BN, linear

  yolo (mask = 3, 4, 5) classes=3 num=9

  route from -4

  conv2d 1x1, 128 channels, stride 1 + bn + leaky

  upsample stride=2

  route from layers -1 and 36  (???)

  #### YOLO output #3:

  conv2d 1x1, 128 channels, stride 1 + bn + leaky |
  conv2d 3x3, 256 channels, stride 1 + bn + leaky |  x 3

  conv2d 1x1, 24 channels, no BN, linear

  yolo (mask = 0, 1, 2) classes=3 num=9


Here is the YOLOv3 paper: https://arxiv.org/pdf/1804.02767.pdf
Main ideas:
  * For each cell there are 9 "dimension clusters" - this is from YOLOv2.
  * Performs multiclass classification - no softmax.
  * Boxes predicted at 3 different scales (this is the reason for the 3 YOLO layers).
    * Predict 3 boxes at each scale. This is why there are three entries in the "masks" field.
  * Bounding box priors based on k-means clustering from the COCO dataset.
  * The classifier network itself (without the YOLO layers) is what is called "DarkNet-53".

Implementation notes:
  - Shortcut layer adds the previous layer to the output of the referenced layer.
  - Route layer seems to just emit the output of the referenced layers.
  - There is a LOT of complexity in the YOLO layer code. Need to dig in and understand
    this better.

----
Getting error with mismatch in shape from the linear-activation conv2d input to Yolo layer.

In the sample code I see the conv2d layer has 24 channels for 3 classes and 9 boxes
In the yolov2-voc.cfg there are 75 channels for 20 classes and 9 boxes
In the yolov2-tiny.cfg there are 255 channels for 80 classes and 6 boxes
In the yolov2-spp.cfg there are 255 channels for 80 classes and 9 boxes
In the yolov2-openimages.cfg there are 1818 channels for 601 classes and 9 boxes
In the yolov2.cfg there are 255 channels for 80 classes and 9 boxes

Seems to be the number is (# classes * 3) + 15.
Not sure where the 15 comes from.
For my 40-class model it would be 135.

---

Interpreting model outputs:

We have 40 classes.

Yolo Layer 1 takes as input a tensor of size [1, 135, 13, 13] and produces a tensor of [1, 507, 45]

The 135 is (40 * 3) + 15.

nA = 3 (number of bounding boxes)
nB = 1 (number of images in batch)
nG = 13 (number of grid cells)
stride = 32 (which is 416 / 13, i.e., size of each grid cell in pixels)

The input is permuted into a tensor of size [1, 3, 13, 13, 45]

So there are 3 bbox predictions for each of the 13x13 grid cells.
The 45 values for each prediction are:
  x (sigmoid applied)
  y (sigmoid applied)
  w
  h
  prediction confidence ("objectness score")
  class pred (one value per class, 40 classes)

These get reshapes into `pred_boxes`, `pred_conf`, and `pred_cls` with sizes:

pred_boxes size is torch.Size([1, 3, 13, 13, 4])
  - x, y, w, h for each box

pred_conf size is torch.Size([1, 3, 13, 13])
  - confidence for each box

pred_cls size is torch.Size([1, 3, 13, 13, 40])
  - per-class class probability for each box

All of these get squashed into a single output tensor like so:

   output = torch.cat(
                (
                    pred_boxes.view(nB, -1, 4) * stride,
                    pred_conf.view(nB, -1, 1),
                    pred_cls.view(nB, -1, self.num_classes),
                ),
                -1,
            )

To use the predictions in my own code I need to un-squash these tensors.

The squashing does this:

MDW: pred_boxes is now torch.Size([1, 507, 4])
MDW: pred_conf is now torch.Size([1, 507, 1])
MDW: pred_cls is now torch.Size([1, 507, 40])

WHich results in a tensor of [1, 507, 45] when they are concatenated.

This code can be used to reverse the concatenation for a single YOLO layer:

  foo = torch.split(output, [4, 1, 40], -1)
  assert torch.all(torch.eq(foo[0].view(nB, 3, nG, nG, 4) / stride, pred_boxes))
  assert torch.all(torch.eq(foo[1].view(nB, 3, nG, nG), pred_conf))
  assert torch.all(torch.eq(foo[2].view(nB, 3, nG, nG, self.num_classes), pred_cls))

---

How do we interpret the bounding box coords produced by the model?

Bounding boxes read by the dataset code convert from
  x y w h
in pixels to

  cx cy sw sh

where (cx, cy) is the center of the bounding box, scaled to the (padded) width and height of the
image -- e.g., between 0.0 and 1.0. sw and sh are scaled to the (padded) size of the image as well.

The input images are 512x512 but are resized to 416x416 in the dataset code.

What is coming out "raw" from the model is (x, y, w, h) with respect to the (fixed) anchor boxes.
The model code seems to scale these back to pixels before spitting them out, I think?
But did they get scaled to (416x416) not (512x512)?

*** Need to look at the 'build_targets' code which does the matching between predicted and target
boxes, to see how it does that scaling and comparison.

For the first YOLO layer, we have 3 anchor boxe sizes (these are width/height):
  [116, 90], [156, 198], [373, 326]

(I don't quite follow why the first layer only considers these three)

In YOLOLayer.forward(), these are scaled by 'stride', which is 32 pixels (for the 13x13 grid size).
Why?
  - This would seem to map the anchor box size to a grid cell
  - However, that is not my understanding of how we should interpret the anchor box sizes!

The model's prediction is shapeed [1, 3, 13, 13, 45], which is:
  For each of the 13x13 grid cells
    - 3 bounding boxes
      - Each of which has 45 parameters
        - x, y, w, h, objectness
        - plus 40 class probabilities

The raw x, y, w, h coming from the model are interpreted as:

  x = torch.sigmoid(raw_x) + grid_x
  y = torch.sigmoid(raw_y) + grid_y
  w = torch.exp(raw_w) * scaled_anchor_w
  h = torch.exp(raw_h) * scaled_anchor_h

So we are now operating in "grid coordinates", ranging from 0 ... 12 (for example)
Each of the 13x13 grid cells is predicting 3 bounding boxes, whose centers
might lie *outside* that particular grid cell (based on the application of the
sigmoid function to the x and y coordinates).

By scaling the anchor box sizes to "grid cell units", that means that the
final 'w' and 'h' values are in terms of some number of grid cells, e.g.,
a 300 pixel wide anchor box on a 416x416 image base with 13x13 cells would
end up spanning (300 / 32) = 9.37 grid cells.

OK, so the final pred_bboxes passed to build_targets are the *center* of the
bbox in *grid cell units*, not pixels and not image units. Okay.

Let's see if this makes any sense.
































